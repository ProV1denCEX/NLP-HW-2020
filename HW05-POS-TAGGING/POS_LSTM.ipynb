{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Due March 17th, 23:59\n",
    "\n",
    "In this homework you will be implementing a LSTM model for POS tagging.\n",
    "\n",
    "You are given the following files:\n",
    "- `POS_NEMM.ipynb`: Notebook file for NEMM model (Optional)\n",
    "- `POS_LTML.ipynb`: Notebook file for MTML model\n",
    "- `train.txt`: Training set to train your model\n",
    "- `test.txt`: Test set to report your modelâ€™s performance\n",
    "- `tags.csv`: Treebank tag universe\n",
    "- `sample_prediction.csv`: Sample file your prediction result should look like\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "\n",
    "### Deliverables (zip them all)\n",
    "\n",
    "- pdf or html version of your final notebook\n",
    "- Use the best model you trained, generate the prediction for test.txt, name the\n",
    "output file prediction.csv (Be careful: the best model in your training set might not\n",
    "be the best model for the test set).\n",
    "- writeup.pdf: summarize the method you used and report their performance.\n",
    "If you worked on the optional task, add the discussion. Add a short essay\n",
    "discussing the biggest challenges you encounter during this assignment and\n",
    "what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "    \n",
    "from utils.hw5 import load_data, save_prediction, ignore_class_accuracy, whole_sentence_accuracy\n",
    "from utils.general import show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tags` is a dictionary that maps the [Treebank tag](https://www.clips.uantwerpen.be/pages/mbsp-tags) to its numerical encoding. There are 45 tags in total, plus a special tag `START (tags[-1])` to indicate the beginning of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:10:05.453780Z",
     "start_time": "2019-04-03T05:10:04.295178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 33539\n",
      "Dev set: 11180\n",
      "Testing set: 9955\n"
     ]
    }
   ],
   "source": [
    "tags = list(pd.read_csv('tags.csv', index_col=0).tag_encode.keys())\n",
    "\n",
    "train, train_label = load_data(\"train.txt\")\n",
    "train, dev, train_label, dev_label = train_test_split(train, train_label)\n",
    "test, _ = load_data(\"test.txt\")\n",
    "\n",
    "print(\"Training set: %d\" % len(train))\n",
    "print(\"Dev set: %d\" % len(dev))\n",
    "print(\"Testing set: %d\" % len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:14:20.197334Z",
     "start_time": "2019-04-03T05:14:20.146236Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class POS_LSTMM:\n",
    "    \"\"\"\n",
    "    To help you focus on the LSTM model, I have made most part of the code ready, make sure you\n",
    "    read all the parts to understand how the code works. You only need to modify the prepare method \n",
    "    to add the RNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, tag_vocab=tags, max_sent_len=40, \n",
    "                 voc_min_freq=5, **kwargs):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            tag_vocab: tag dictionary, you will less likely need to change this\n",
    "            voc_min_freq: use this to truncate low frequency vocabulary\n",
    "            max_sent_len: truncate/pad all sentences to this length\n",
    "            \n",
    "            kwargs: Use as needed to pass extra parameters\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.reverse_vocab = {}\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.reverse_tag_vocab = {k:v for v, k in enumerate(tag_vocab)}\n",
    "        self._voc_min_freq = voc_min_freq\n",
    "        self._max_sent_len = max_sent_len\n",
    "\n",
    "        \"\"\"\n",
    "        Feel free to add code here as you need\n",
    "        \"\"\"\n",
    "\n",
    "    def collect_vocab(self, X):\n",
    "        \"\"\"\n",
    "        Create vocabulary from all input data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "        \"\"\"\n",
    "        vocab = Counter([t for s in X for t in s])\n",
    "        vocab = {k: v for k, v in vocab.items() if v > self._voc_min_freq}\n",
    "        vocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab, key=lambda x: vocab[x], reverse=True)\n",
    "        reverse_vocab = {k: v for v, k in enumerate(vocab)}\n",
    "        \n",
    "        return vocab, reverse_vocab\n",
    "                \n",
    "    def transform_X(self, X):\n",
    "        \"\"\"\n",
    "        Translate input raw data X into trainable numerical data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "        \"\"\"\n",
    "        X_out = []\n",
    "        \n",
    "        default = self.reverse_vocab['<UNK>']\n",
    "        for sent in X:\n",
    "            X_out.append([self.reverse_vocab.get(t, default) for t in sent])\n",
    "            \n",
    "        X_out = pad_sequences(sequences=X_out, maxlen=self._max_sent_len, \n",
    "                              padding='post', truncating='post',\n",
    "                              value=self.reverse_vocab['<PAD>'])\n",
    "        \n",
    "        return X_out\n",
    "    \n",
    "    def transform_Y(self, Y):\n",
    "        \"\"\"\n",
    "        Translate input raw data Y into trainable numerical data\n",
    "        input:\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        Y_out = [] \n",
    "        \n",
    "        for labs in Y:\n",
    "            Y_out.append([self.reverse_tag_vocab[lab] for lab in labs])\n",
    "            \n",
    "        Y_out = pad_sequences(sequences=Y_out, maxlen=self._max_sent_len, \n",
    "                              padding='post', truncating='post',\n",
    "                              value=self.reverse_tag_vocab['<PAD>'])\n",
    "        \n",
    "        return Y_out\n",
    "    \n",
    "    def prepare(self, X, Y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            X: list of sentences\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        self.vocab, self.reverse_vocab = self.collect_vocab(X)\n",
    "        X, Y = self.transform_X(X), self.transform_Y(Y)\n",
    "        \n",
    "        embedding_dim = 100\n",
    "        lstm_node = 128\n",
    "\n",
    "        \"\"\"\n",
    "        Write your own model here\n",
    "        Hints:\n",
    "            - Rember to use embedding layer at the beginning\n",
    "            - Use Bidrectional LSTM to take advantage of both direction history  \n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self._max_sent_len,)))\n",
    "        model.add(Embedding(len(self.vocab), embedding_dim))\n",
    "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(TimeDistributed(Dense(len(self.tag_vocab))))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        \"\"\"\n",
    "        You can read the source code to understand how ignore_class_accuracy works.\n",
    "        The reason of using this customized metric is because we have padded the training \n",
    "        data with lots of '<PAD>' tag. It's easy and useless to predict this tag, we need \n",
    "        to ignore this tag when calculate the accuracy.\n",
    "        \"\"\"\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(0.001),\n",
    "                      metrics=['accuracy', \n",
    "                               ignore_class_accuracy(self.reverse_tag_vocab['<PAD>']),\n",
    "                               whole_sentence_accuracy(self.reverse_tag_vocab['<PAD>'])])\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, batch_size=128, epochs=10):\n",
    "        X, Y = self.transform_X(X), self.transform_Y(Y)\n",
    "        self.model.fit(X, to_categorical(Y, num_classes=len(self.tag_vocab)),\n",
    "                       batch_size=batch_size, \n",
    "                       epochs=epochs, validation_split=0.2)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X_new = self.transform_X(X)\n",
    "        Y_pred = self.model.predict_classes(X_new)\n",
    "    \n",
    "        for i, y in enumerate(Y_pred):\n",
    "            results.append(\n",
    "                [self.tag_vocab[y[j]] for j in range(min(len(X[i]), len(X_new[i])))]\n",
    "            )\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a POS_LSTM model with two Dropout layer to prevent overfitting during model training. Two Dropout layers have the same hyperparameter as 20% dropout rate, which means at every training process there are 20% of nodes to be ignored and set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 100)           803900    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40, 256)           234496    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 47)            12079     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 47)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 47)            0         \n",
      "=================================================================\n",
      "Total params: 1,050,475\n",
      "Trainable params: 1,050,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = POS_LSTMM().prepare(train, train_label)\n",
    "lstm.model.summary()\n",
    "# show_keras_model(lstm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26831 samples, validate on 6708 samples\n",
      "Epoch 1/10\n",
      "26831/26831 [==============================] - 7s 261us/sample - loss: 1.7356 - accuracy: 0.5314 - ignore_accuracy: 0.2222 - sentence_accuracy: 0.0054 - val_loss: 0.7562 - val_accuracy: 0.8282 - val_ignore_accuracy: 0.6259 - val_sentence_accuracy: 0.0541\n",
      "Epoch 2/10\n",
      "26831/26831 [==============================] - 4s 141us/sample - loss: 0.8139 - accuracy: 0.7442 - ignore_accuracy: 0.6793 - sentence_accuracy: 0.0582 - val_loss: 0.1583 - val_accuracy: 0.9619 - val_ignore_accuracy: 0.9171 - val_sentence_accuracy: 0.3075\n",
      "Epoch 3/10\n",
      "26831/26831 [==============================] - 4s 136us/sample - loss: 0.5981 - accuracy: 0.7809 - ignore_accuracy: 0.7591 - sentence_accuracy: 0.0833 - val_loss: 0.1005 - val_accuracy: 0.9706 - val_ignore_accuracy: 0.9359 - val_sentence_accuracy: 0.4000cura\n",
      "Epoch 4/10\n",
      "26831/26831 [==============================] - 4s 137us/sample - loss: 0.5626 - accuracy: 0.7863 - ignore_accuracy: 0.7702 - sentence_accuracy: 0.0882 - val_loss: 0.0874 - val_accuracy: 0.9730 - val_ignore_accuracy: 0.9410 - val_sentence_accuracy: 0.4226\n",
      "Epoch 5/10\n",
      "26831/26831 [==============================] - 4s 133us/sample - loss: 0.5494 - accuracy: 0.7889 - ignore_accuracy: 0.7764 - sentence_accuracy: 0.0950 - val_loss: 0.0822 - val_accuracy: 0.9737 - val_ignore_accuracy: 0.9426 - val_sentence_accuracy: 0.4319\n",
      "Epoch 6/10\n",
      "26831/26831 [==============================] - 4s 133us/sample - loss: 0.5426 - accuracy: 0.7898 - ignore_accuracy: 0.7776 - sentence_accuracy: 0.0954 - val_loss: 0.0793 - val_accuracy: 0.9743 - val_ignore_accuracy: 0.9439 - val_sentence_accuracy: 0.4369\n",
      "Epoch 7/10\n",
      "26831/26831 [==============================] - 4s 137us/sample - loss: 0.5370 - accuracy: 0.7910 - ignore_accuracy: 0.7814 - sentence_accuracy: 0.0989 - val_loss: 0.0778 - val_accuracy: 0.9748 - val_ignore_accuracy: 0.9450 - val_sentence_accuracy: 0.4509\n",
      "Epoch 8/10\n",
      "26831/26831 [==============================] - 4s 135us/sample - loss: 0.5330 - accuracy: 0.7918 - ignore_accuracy: 0.7821 - sentence_accuracy: 0.0998 - val_loss: 0.0764 - val_accuracy: 0.9754 - val_ignore_accuracy: 0.9464 - val_sentence_accuracy: 0.4669\n",
      "Epoch 9/10\n",
      "26831/26831 [==============================] - 4s 136us/sample - loss: 0.5288 - accuracy: 0.7928 - ignore_accuracy: 0.7843 - sentence_accuracy: 0.1007 - val_loss: 0.0762 - val_accuracy: 0.9755 - val_ignore_accuracy: 0.9466 - val_sentence_accuracy: 0.4615curacy: 0.7843 - sentence_accuracy: 0.10 - ETA: 0s - loss: 0.5285 - accuracy: 0.7929 - ignore_accuracy: 0.7843 - sentence_ac\n",
      "Epoch 10/10\n",
      "26831/26831 [==============================] - 4s 136us/sample - loss: 0.5263 - accuracy: 0.7930 - ignore_accuracy: 0.7853 - sentence_accuracy: 0.1023 - val_loss: 0.0764 - val_accuracy: 0.9754 - val_ignore_accuracy: 0.9464 - val_sentence_accuracy: 0.4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.POS_LSTMM at 0x295b7980d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = POS_LSTMM().prepare(train, train_label)\n",
    "lstm.fit(train, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:15:01.277528Z",
     "start_time": "2019-04-03T05:14:59.503776Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = lstm.predict(test)\n",
    "save_prediction(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
