{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>HW2 Convolutional neural networks for text classification\n",
    "</center></h1>\n",
    "<h2><center>Due: March 2020 13th, 23:59</center></h2>\n",
    "\n",
    "In this homework you will learn how to build a simple convolutional neural networks (1 convolution layer with max pooling + 1 activation layer) from scratch, and use the model to solve text classification problem. As optional, you also have a chance to build real life CNN models using Keras + Tensorflow and use it to challenge the model you build from scratch. \n",
    "    \n",
    "<h3> 1. Math preliminaries </h3>\n",
    "\n",
    "Please answer all these questions:\n",
    "    \n",
    "1. What is the form of sigmoid function $σ(z)$ ? Show that $σ′(z) = σ(z)[1 − σ(z)]$.\n",
    "<br/>\n",
    "\n",
    "2. Another popular activation function is $tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$ , show that $tanh′(z) = 1 − tanh(z)^2$.\n",
    "<br/>\n",
    "\n",
    "3. For a single variable single layer perceptron with sigmoid activation function (equivalent\n",
    "to LR) and loss function defined as:\n",
    "<center>$\\hat{y}_i = σ ( w_1 x_i + w_0 )$ </center>\n",
    "<center>$L(w_0, w_1) = \\sum_i y_i lg(\\hat{y}_i)+(1−y_i)lg(1−\\hat{y}_i)$ </center>\n",
    "Show that:\n",
    "<center>$\\frac{∂L}{∂w_1} =\\sum_i(y_i−\\hat{y}_i)x_i$ </center>\n",
    "<center>$\\frac{∂L}{∂w_0} =\\sum_i(y−\\hat{y}_i)$  </center>\n",
    "<br/>\n",
    "\n",
    "4. For column vectors $\\vec{x}$ and $\\vec{w}$ , and a symmetric matrix $\\overleftrightarrow{M}$, define the gradient operator:\n",
    "<center> $∇_\\vec{x} = (\\frac{∂}{∂x_0}, \\frac{∂}{∂x_1}, ...,\\frac{∂}{∂x_n})^T$ </center>\n",
    "show that:\n",
    "<center> $∇_x(\\vec{w}^T\\vec{x}) = \\vec{w}$ </center>\n",
    "<center> $∇_x(\\vec{x}^T\\vec{w}) = \\vec{w}$ </center>\n",
    "<center> $∇_x(\\vec{w}^T\\overleftrightarrow{M}\\vec{x}) = \\overleftrightarrow{M}\\vec{w}$ </center>\n",
    "<br/>\n",
    "\n",
    "5. Let’s expand Q3 to a more general case. Suppose there is a single layer perceptron with multiple variables:\n",
    "<center> $\\hat{y}_i = σ( \\vec{w}^T \\vec{x_i} )$ </center>\n",
    "<center>$L(\\vec{w}) = \\sum_i y_i lg(\\hat{y}_i)+(1−y_i)lg(1−\\hat{y}_i)$ </center>\n",
    "show that:\n",
    "<center> $∇_\\vec{w}L(\\vec{w}) = \\sum_i(y_i - \\hat{y}_i)\\vec{x_i}$ </center>\n",
    "(hint: use the notation defined in Q4)\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "6. In a CNN illustrated as Fig 1, suppose the loss function is:\n",
    "<center> $L(\\overleftrightarrow{U}, \\vec{w}) = \\sum_i y_i lg(\\hat{y}_i)+(1−y_i)lg(1−\\hat{y}_i)$ </center>\n",
    "From the conclusion in Q5, we can get that:\n",
    "<center> $∇_w L(\\overleftrightarrow{U}, \\vec{w}) = \\sum_i (y_i -\\hat{y}_i)\\vec{h}^{(i)}$ </center>\n",
    "Can you calculate $∇_{u_i} L(U,w)$ using similar techniques?\n",
    "\n",
    "<img src=\"CNN.png\" style=\"width:700px\">\n",
    "\n",
    "<h3> 2. Coding exercise </h3>\n",
    "\n",
    "Follow the instruction in the notebook, and implement the missing code to build the CNN classifier from scratch. Note that the training might be very slow. Consider reducing the training data size and vocabulary size for testing your code. Ask questions in Piazza if you get blocked.\n",
    "\n",
    "Hint: In this CNN, words should be one-hot encoded, but we actually numerically encoded it in the code. This is a sparse trick we did to boost the efficiency, try to understand how it works.\n",
    "\n",
    "Some of the key details you will have a chance to implement:\n",
    "- Forward propagation of a CNN network\n",
    "- Backward propagation of a CNN network\n",
    "- Numerical gradient checking \n",
    "- Use Keras and TensorFlow to implement more complex CNN networks\n",
    "    \n",
    "You are given the following files:\n",
    "- `hw02.ipynb`: Notebook file with starter code\n",
    "- `train.txt`: Training set to train your model\n",
    "- `test.txt`: Test set to report your model’s performance\n",
    "- `sample_prediction.csv`: Sample file your prediction result should look like\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "<h3> 3. Deliverables (zip them all) </h3>\n",
    "\n",
    "- pdf version of your final notebook.\n",
    "- Use the best model you trained, generate the prediction for test.txt, name the\n",
    "output file prediction.csv (Be careful: the best model in your training set might not\n",
    "be the best model for the test set).\n",
    "- After you finished the run, does the model perform better than the bag of words\n",
    "model you built last week? What do you think that contributes to the difference in\n",
    "performance?\n",
    "- HW1_writeup.pdf: summarize the method you used and report their performance.\n",
    "If you worked on the optional task, add the discussion. Add a short essay\n",
    "discussing the biggest challenges you encounter during this assignment and\n",
    "what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== Coding Starts Here ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T22:47:18.404322Z",
     "start_time": "2020-03-01T22:47:18.235083Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "\n",
    "from utils.hw2 import load_data, save_prediction, read_vocab\n",
    "from utils.general import sigmoid, tanh, show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:40:29.169465Z",
     "start_time": "2019-03-18T04:40:29.086590Z"
    }
   },
   "source": [
    "# CNN model \n",
    "Complete the code block in the cells in this section.\n",
    "\n",
    "* step1: Implement the pipeline method to process the raw input\n",
    "* step2: Implement the forward method\n",
    "* step3: Implement the backward method\n",
    "* step4: Run the cell below to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T07:00:18.632501Z",
     "start_time": "2019-03-19T06:47:59.260497Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell shows you how the model will be used, you have to finish the cell below before you\n",
    "can run this cell. \n",
    "\n",
    "Once the implementation is done, you should hype tune the parameters to find the best config\n",
    "\n",
    "Note I only selected 2000 data points to speed up debugging, you should use all the data to train the \n",
    "final model\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_data(\"train.txt\")\n",
    "vocab = read_vocab(\"vocab.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X[:2000], y[:2000], test_size=0.3)\n",
    "cls = CNNTextClassificationModel(vocab)\n",
    "cls.train(X_train, y_train, X_dev, y_dev, nEpoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T00:54:57.880092Z",
     "start_time": "2020-03-12T00:54:57.598477Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CNNTextClassificationModel:\n",
    "    def __init__(self, vocab, window_size=2, F=100, alpha=0.1):\n",
    "        \"\"\"\n",
    "        F: number of filters\n",
    "        alpha: back propagatoin learning rate\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.F = F\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # U and w are the weights of the hidden layer, see Fig 1 in the pdf file\n",
    "        # U is the 1D convolutional layer with shape: voc_size * num_filter * window_size\n",
    "        self.U = np.random.normal(loc=0, scale=0.01, size=(len(vocab), F, window_size))\n",
    "        # w is the weights of the activation layer (after max pooling)\n",
    "        self.w = np.random.normal(loc=0, scale=0.01, size=(F + 1))\n",
    "        \n",
    "    def pipeline(self, X):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to:\n",
    "        1. Tokenize, Normalize the raw input\n",
    "        2. Translate raw data input into numerical encoded vectors\n",
    "        \n",
    "        :param X: raw data input\n",
    "        :return: list of lists\n",
    "        \n",
    "        For example:\n",
    "        X = [[\"Apples orange banana\"]\n",
    "         [\"orange apple bananas\"]] \n",
    "        returns:\n",
    "        [[0, 1, 2], \n",
    "         [1, 0, 2]]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        X2 = []\n",
    "            \n",
    "        return X2\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(probs, labels):\n",
    "        assert len(probs) == len(labels), \"Wrong input!!\"\n",
    "        a = np.array(probs)\n",
    "        b = np.array(labels)\n",
    "        \n",
    "        return 1.0 * (a==b).sum() / len(b) \n",
    "          \n",
    "    def train(self, X_train, y_train, X_dev, y_dev, nEpoch=50):\n",
    "        \"\"\"\n",
    "        Function to fit the model\n",
    "        :param X_train, X_dev: raw data input\n",
    "        :param y_train, y_dev: label \n",
    "        :nEpoch: number of training epoches\n",
    "        \"\"\"\n",
    "        X_train = self.pipeline(X_train)\n",
    "        X_dev = self.pipeline(X_dev)\n",
    "        \n",
    "        for epoch in range(nEpoch):\n",
    "            self.fit(X_train, y_train)\n",
    "            \n",
    "            accuracy_train = self.accuracy(self.predict(X_train), y_train)\n",
    "            accuracy_dev = self.accuracy(self.predict(X_dev), y_dev)\n",
    "            \n",
    "            print(\"Epoch: {}\\tTrain accuracy: {:.3f}\\tDev accuracy: {:.3f}\"\n",
    "                  .format(epoch, accuracy_train, accuracy_dev))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: numerical encoded input\n",
    "        \"\"\"\n",
    "        for (data, label) in zip(X, y):\n",
    "            self.backward(data, label)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: numerical encoded input\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for data in X:\n",
    "            if self.forward(data)[\"prob\"] > 0.5:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def forward(self, word_indices):\n",
    "        \"\"\"\n",
    "        :param word_indices: a list of numerically ecoded words\n",
    "        :return: a result dictionary containing 3 items -\n",
    "        result['prob']: \\hat y in Fig 1.\n",
    "        result['h']: the hidden layer output after max pooling, h = [h1, ..., hf]\n",
    "        result['hid']: argmax of F filters, e.g. j of x_j\n",
    "        e.g. for the ith filter u_i, tanh(word[hid[j], hid[j] + width]*u_i) = h_i\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(word_indices) >= self.window_size, \"Input length cannot be shorter than the window size\"\n",
    "        \n",
    "        h = np.zeros(self.F + 1, dtype=float)\n",
    "        hid = np.zeros(self.F, dtype=int)\n",
    "        prob = 0.0\n",
    "\n",
    "        # layer 1. compute h and hid\n",
    "        # loop through the input data of word indices and\n",
    "        # keep track of the max filtered value h_i and its position index x_j\n",
    "        # h_i = max(tanh(weighted sum of all words in a given window)) over all windows for u_i\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "            \n",
    "        # layer 2. compute probability\n",
    "        # once h and hid are computed, compute the probabiliy by sigmoid(h^TV)\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "    \n",
    "        # return result\n",
    "        return {\"prob\": prob, \"h\": h, \"hid\": hid}\n",
    "    \n",
    "    def backward(self, word_indices, label):\n",
    "        \"\"\"\n",
    "        Update the U, w using backward propagation\n",
    "        \n",
    "        :param word_indices: a list of numerically ecoded words\n",
    "        :param label: int 0 or 1\n",
    "        :return: None\n",
    "        \n",
    "        update weight matrix/vector U and V based on the loss function\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = self.forward(word_indices)\n",
    "        prob = pred[\"prob\"]\n",
    "        h = pred[\"h\"]\n",
    "        hid = pred[\"hid\"]\n",
    "\n",
    "        # update U and w here\n",
    "        # to update V: w_new = w_current + d(loss_function)/d(w)*alpha\n",
    "        # to update U: U_new = U_current + d(loss_function)/d(U)*alpha\n",
    "        # Hint: use Q6 in the first part of your homework\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "\n",
    "    def calc_gradients_w(self, pred, y):\n",
    "        return (y - pred['prob']) * pred['h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Build your model using Keras + Tensorflow\n",
    "\n",
    "So far we have always forced you to implement things from scratch. You may feel it's overwhelming, but fortunately, it is not how the real world works. In the real world, there are existing tools you can leverage, so you can focus on the most innovative part of your work. We asked you to do all the previous execises for learning purpose, and since you have already reached so far, it's time to unleash yourself and allow you the access to the real world toolings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:06:30.995290Z",
     "start_time": "2019-03-20T05:06:30.927192Z"
    }
   },
   "outputs": [],
   "source": [
    "# First let's see how you can build a similar CNN model you just had using Keras\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:03:13.785839Z",
     "start_time": "2019-03-20T05:03:13.732270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yes! it is a good practice to do data processing outside the ML model\n",
    "wnet = WordNetLemmatizer()\n",
    "# Numerical encode all the words\n",
    "unknown = vocab['__unknown__']\n",
    "X_train2 = [[vocab.get(wnet.lemmatize(w), unknown) for w in word_tokenize(sent)] for sent in X_train]\n",
    "X_dev2 = [[vocab.get(wnet.lemmatize(w), unknown)for w in word_tokenize(sent)] for sent in X_dev]\n",
    "\n",
    "# Tensorflow does not handle variable length input well, let's unify all input to the same length\n",
    "def trim_X(X, max_length=100, default=vocab['.']):\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i]) > max_length:\n",
    "            X[i] = X[i][:max_length]\n",
    "        elif len(X[i]) < max_length:\n",
    "            X[i] = X[i] + [default] * (max_length - len(X[i]))\n",
    "            \n",
    "    return np.array(X)\n",
    "            \n",
    "X_train2 = trim_X(X_train2, MAX_LENGTH)\n",
    "X_dev2 = trim_X(X_dev2, MAX_LENGTH)\n",
    "\n",
    "\n",
    "# Now we have all the input data nicely encoded with numerical label, and each of the input data are trimmed \n",
    "# to have the same length. We would have needed to further apply one-hot encode for each word. However, this \n",
    "# would be very expensive, since each word will be expanded into a len(vocab) (~10000) length vector. Keras does\n",
    "# not support sparse matrix input at this moment. But don't worry, we will use an advanced technique called embedding\n",
    "# layer. This concept will be introduced in the next lesson. At this moment, you don't have to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:38:00.091414Z",
     "start_time": "2019-03-20T05:37:59.875258Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(vocab), input_length=MAX_LENGTH, output_dim=1024, name=\"Embedding-1\"))\n",
    "model.add(Conv1D(filters=100, kernel_size=2, activation=\"tanh\", name=\"Conv1D-1\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"MaxPooling1D-1\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"Dense-1\"))\n",
    "print(model.summary())\n",
    "\n",
    "show_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:39:27.542489Z",
     "start_time": "2019-03-20T05:38:02.612896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train, epochs=10, validation_data=[X_dev2, y_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown you have to use an industry level tool to build a CNN model. Hopefully you think it is simpler than the version we built from scratch. Not really? Read Keras Documentation and learn more: https://keras.io/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T07:24:16.164784Z",
     "start_time": "2019-03-19T07:24:16.104613Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Now it's your turn to build some more complicated CNN models\n",
    "\n",
    "\"\"\"\n",
    "Implement your code here\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
