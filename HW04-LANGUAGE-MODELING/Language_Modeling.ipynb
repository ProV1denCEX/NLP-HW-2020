{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> <font color=\"red\"> STAY SAFE !! </font> </center></h1>\n",
    "\n",
    "\n",
    "<h1><center>HW4 Language Modeling (LM)</center></h1>\n",
    "<h2><center>Due: April 2020 10th, 23:59</center></h2>\n",
    "\n",
    "In this homework, you will first implement a simple bigram language model on a dataset containing news headlines, learn basic concepts of marcov modeling, words sampling, and perplexity. \n",
    "\n",
    "Then things start get very fun and open ended. You will be shown a simple word based RNN LM. Understand how it works, and then apply changes to it as you wish. Things you can try but not limited to:\n",
    "\n",
    "1. Word based RNN model with subword embedding\n",
    "2. Character based RNN model\n",
    "2. Try different model architecture\n",
    "3. Try different training corprus\n",
    "4. Personalized LM\n",
    "\n",
    "**You are given the following files**:\n",
    "- `Language_Modeling.ipynb`: Notebook file with starter code\n",
    "- `headlines.train`: Training set to train your model\n",
    "- `headlines.dev`: Test set to report your model’s performance\n",
    "- `glove_300d.csv`: Glove embedding truncated for the vocab in the training data\n",
    "- `../utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "**Deriverables**:\n",
    "- pdf or html of the notebook\n",
    "- A report of your own model if you have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T19:52:22.037603Z",
     "start_time": "2020-03-15T19:52:21.343054Z"
    }
   },
   "source": [
    "### ======================== Coding starts here ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:44.068513Z",
     "start_time": "2020-03-22T20:39:33.927889Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os, sys, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "\n",
    "from utils.hw4 import (load_data, load_data_char, gen_vocab, START, END, UNK, \n",
    "                       load_embedding)\n",
    "from utils.general import sigmoid, tanh, show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:45.386244Z",
     "start_time": "2020-03-22T20:39:44.072238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>ex westpac worker banned from financial services<END>\n",
      "<START>red gums must not be locked up williams<END>\n",
      "<START>mars methane a sign of life scientists<END>\n",
      "<START>property raided in van tongeren probe<END>\n",
      "<START>serbia calls for more kosovo talks<END>\n"
     ]
    }
   ],
   "source": [
    "# The input is trucated for fast iteration\n",
    "# Remember to use the full set of data for your final model training\n",
    "# It may take some time\n",
    "headlines_train = load_data(\"headlines.train\")[:10000]\n",
    "headlines_dev = load_data(\"headlines.dev\")[:100]\n",
    "\n",
    "# Before we begin, let's look at what some of the headlines look like. \n",
    "# Run the following code block as many times as you want to get a sense \n",
    "# of what kind of headlines we hope to generate.\n",
    "for headline in random.sample(headlines_train, 5):\n",
    "    print(START + ' '.join(headline) + END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.040257Z",
     "start_time": "2020-03-22T20:39:45.406419Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  3152\n",
      "Longest setence length:  15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>to</th>\n",
       "      <th>in</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>on</th>\n",
       "      <th>over</th>\n",
       "      <th>the</th>\n",
       "      <th>...</th>\n",
       "      <th>whats</th>\n",
       "      <th>crown</th>\n",
       "      <th>assessment</th>\n",
       "      <th>limits</th>\n",
       "      <th>sir</th>\n",
       "      <th>effect</th>\n",
       "      <th>victorias</th>\n",
       "      <th>2011</th>\n",
       "      <th>rodeo</th>\n",
       "      <th>insists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.347303</td>\n",
       "      <td>0.269341</td>\n",
       "      <td>0.319768</td>\n",
       "      <td>-0.248370</td>\n",
       "      <td>0.068507</td>\n",
       "      <td>-0.23909</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>-0.089285</td>\n",
       "      <td>-0.208380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013198</td>\n",
       "      <td>0.48025</td>\n",
       "      <td>0.185170</td>\n",
       "      <td>-0.178330</td>\n",
       "      <td>-0.42118</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>-0.22978</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>-0.118620</td>\n",
       "      <td>0.054069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350850</td>\n",
       "      <td>-0.513849</td>\n",
       "      <td>1.111513</td>\n",
       "      <td>-0.454610</td>\n",
       "      <td>-0.023344</td>\n",
       "      <td>-0.64189</td>\n",
       "      <td>-0.285920</td>\n",
       "      <td>0.048631</td>\n",
       "      <td>-0.077838</td>\n",
       "      <td>-0.149320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>-0.16388</td>\n",
       "      <td>0.089710</td>\n",
       "      <td>-0.267100</td>\n",
       "      <td>-0.25707</td>\n",
       "      <td>-0.210200</td>\n",
       "      <td>0.28016</td>\n",
       "      <td>-0.154070</td>\n",
       "      <td>-0.553160</td>\n",
       "      <td>0.348970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001536</td>\n",
       "      <td>-0.142948</td>\n",
       "      <td>0.683539</td>\n",
       "      <td>0.039227</td>\n",
       "      <td>0.282710</td>\n",
       "      <td>-0.58322</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.489690</td>\n",
       "      <td>-0.295420</td>\n",
       "      <td>-0.017528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251280</td>\n",
       "      <td>-0.30821</td>\n",
       "      <td>0.314160</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.12975</td>\n",
       "      <td>-0.140060</td>\n",
       "      <td>-0.75673</td>\n",
       "      <td>0.300470</td>\n",
       "      <td>0.035334</td>\n",
       "      <td>0.149480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.167915</td>\n",
       "      <td>-0.151135</td>\n",
       "      <td>0.613468</td>\n",
       "      <td>-0.284220</td>\n",
       "      <td>-0.402150</td>\n",
       "      <td>-0.54743</td>\n",
       "      <td>-0.601220</td>\n",
       "      <td>0.427770</td>\n",
       "      <td>-0.335840</td>\n",
       "      <td>-0.028432</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008379</td>\n",
       "      <td>0.21813</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>-0.029465</td>\n",
       "      <td>-0.21472</td>\n",
       "      <td>0.041901</td>\n",
       "      <td>0.73045</td>\n",
       "      <td>0.088875</td>\n",
       "      <td>-0.120390</td>\n",
       "      <td>0.267870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.324277</td>\n",
       "      <td>-0.494114</td>\n",
       "      <td>0.830225</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>0.077815</td>\n",
       "      <td>0.42386</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.386100</td>\n",
       "      <td>0.287430</td>\n",
       "      <td>-0.060104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121040</td>\n",
       "      <td>-0.20657</td>\n",
       "      <td>-0.415350</td>\n",
       "      <td>-0.167500</td>\n",
       "      <td>-0.14461</td>\n",
       "      <td>0.304680</td>\n",
       "      <td>0.10413</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      <UNK>   <START>     <END>        to        in      for        of  \\\n",
       "0 -0.347303  0.269341  0.319768 -0.248370  0.068507 -0.23909 -0.036429   \n",
       "1  0.350850 -0.513849  1.111513 -0.454610 -0.023344 -0.64189 -0.285920   \n",
       "2  0.001536 -0.142948  0.683539  0.039227  0.282710 -0.58322  0.063387   \n",
       "3 -0.167915 -0.151135  0.613468 -0.284220 -0.402150 -0.54743 -0.601220   \n",
       "4 -0.324277 -0.494114  0.830225 -0.031852  0.077815  0.42386 -0.015309   \n",
       "\n",
       "         on      over       the  ...     whats    crown  assessment    limits  \\\n",
       "0  0.000607 -0.089285 -0.208380  ... -0.013198  0.48025    0.185170 -0.178330   \n",
       "1  0.048631 -0.077838 -0.149320  ...  0.154500 -0.16388    0.089710 -0.267100   \n",
       "2  0.489690 -0.295420 -0.017528  ... -0.251280 -0.30821    0.314160  0.187700   \n",
       "3  0.427770 -0.335840 -0.028432  ... -0.008379  0.21813    0.006081 -0.029465   \n",
       "4 -0.386100  0.287430 -0.060104  ...  0.121040 -0.20657   -0.415350 -0.167500   \n",
       "\n",
       "       sir    effect  victorias      2011     rodeo   insists  \n",
       "0 -0.42118  0.139860   -0.22978  0.163300 -0.118620  0.054069  \n",
       "1 -0.25707 -0.210200    0.28016 -0.154070 -0.553160  0.348970  \n",
       "2  0.12975 -0.140060   -0.75673  0.300470  0.035334  0.149480  \n",
       "3 -0.21472  0.041901    0.73045  0.088875 -0.120390  0.267870  \n",
       "4 -0.14461  0.304680    0.10413  0.146600  0.167000  0.018603  \n",
       "\n",
       "[5 rows x 3152 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the vocab list and the embedding \n",
    "# It (might) be helpful to remove low frequency words, so the model learns how to\n",
    "# treat unseen vocabulary\n",
    "vocab, re_vocab = gen_vocab(headlines_train, 4)\n",
    "sent_len = max([len(s) for s in headlines_train]) + 1\n",
    "\n",
    "print(\"Size of vocab: \", len(vocab))\n",
    "print(\"Longest setence length: \", sent_len)\n",
    "\n",
    "# Load the embedding, trick is played to fill the missing vocab\n",
    "# you can look into the source file to see what it actually does\n",
    "# This embedding file is truncated for vocab used in this dataset\n",
    "# If you are to train your own model with your own data, remember to download\n",
    "# the original embedding here: https://nlp.stanford.edu/projects/glove/\n",
    "glove = load_embedding('glove_300d.csv', vocab=vocab)\n",
    "\n",
    "glove.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.155696Z",
     "start_time": "2020-03-22T20:39:55.044811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the DF to np array\n",
    "glove = glove.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.273785Z",
     "start_time": "2020-03-22T20:39:55.159796Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_label(token):\n",
    "    \"\"\"\n",
    "    Simply transfer a token to its numerical label, if the token is not int\n",
    "    the vocab, return the label of UNK\n",
    "    input: \n",
    "        token: str\n",
    "        \n",
    "    output:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return re_vocab.get(token, re_vocab[UNK])\n",
    "\n",
    "def to_embedding(X):\n",
    "    \"\"\"\n",
    "    For the 2 dimensional input X filled with the vocabulary label, \n",
    "    return an np.array of their embedding\n",
    "    \n",
    "    input:\n",
    "        X: np.array(n_sample, sent_len)\n",
    "        \n",
    "    return:\n",
    "        embdding\n",
    "    \"\"\"\n",
    "    embedding = np.zeros((len(X), len(X[0]), glove.shape[1]))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            embedding[i,j,:] = glove[X[i][j]] \n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def sample_with_weight(prob, avoid_UNK=True):\n",
    "    \"\"\"\n",
    "    For a given probability distribution, return a random int sampled by the\n",
    "    probability distribution. \n",
    "    \n",
    "    input:\n",
    "        prob: list of float probability\n",
    "        avoid_UNK: boolean, if UNK should be excluded\n",
    "    \"\"\"\n",
    "    unk_idx = re_vocab[UNK]\n",
    "    \n",
    "    if avoid_UNK: \n",
    "        prob[unk_idx] = 0 # Make sure we do not use UNK in the generated text\n",
    "    \n",
    "    # If the distribution is 0, use uniform distribution\n",
    "    if prob.sum() <= 0:\n",
    "        prob[1:] = 1.0\n",
    "        \n",
    "    return np.random.choice(range(len(prob)), p=prob/prob.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-gram (second order) Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FNN  Bi-Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.259783Z",
     "start_time": "2020-03-22T20:39:55.280568Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "Dense-1 (Dense)              (None, 100)               60100     \n",
      "_________________________________________________________________\n",
      "Dense-2 (Dense)              (None, 3152)              318352    \n",
      "=================================================================\n",
      "Total params: 378,452\n",
      "Trainable params: 378,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "\n",
    "n_gram = 2\n",
    "\n",
    "# For simplicity, we use the embedding of words to feed the model, therefore\n",
    "# no need to add a Embedding layer in the begining. But for a possibly better performance\n",
    "# you can add a embedding layer, even better if you use the glove embedding matrix as the\n",
    "# initial value for the embedding layer\n",
    "# This is useful also because we have filled the embedding with random values for those missing\n",
    "# vocabularies, allowing the embedding matrix to relax during training will improve the performance \n",
    "# for these words as well. But be prepared that this would slow down the training\n",
    "FNN_model = Sequential()\n",
    "FNN_model.add(Reshape(target_shape=(n_gram * glove.shape[1],), \n",
    "                      input_shape=(n_gram, glove.shape[1],)))\n",
    "FNN_model.add(Dense(100, activation=\"relu\", name=\"Dense-1\"))\n",
    "FNN_model.add(Dense(len(vocab), activation=\"softmax\", name=\"Dense-2\"))\n",
    "\n",
    "FNN_model.summary()\n",
    "# show_keras_model(FNN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.387115Z",
     "start_time": "2020-03-22T20:39:56.263841Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "def gen_sample_FNN(data, batch_size=1000, one_hot=True):\n",
    "    \"\"\"\n",
    "    For training the model, we need to shift the data by -1 to produce\n",
    "    label, i.g.\n",
    "    [\"word1\", \"word2\", \"word3\", \"word4\"] --> \n",
    "    X: [[START, STSRT],\n",
    "        [START, 1],\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 4]]\n",
    "    Y: [1, 2, 3, 4, ...] if one_hot is False, the label is translated to \n",
    "        one-hot if ont_hot is True\n",
    "    \n",
    "    inputs:\n",
    "        data: list of list of strings\n",
    "        batch_size: int\n",
    "        one_hot: boolean\n",
    "        \n",
    "    outputs:\n",
    "        X: np.array(batch_size, n_gram, embedding_dim)\n",
    "        Y: np.array(batch_size, ) or np.array(batch_size, vocab_size)\n",
    "        \n",
    "    batch size is used to control the size for each data batch\n",
    "    set batch_size = -1 if you don't want to generate by batch\n",
    "    \"\"\"\n",
    "    if batch_size == -1:\n",
    "        batch_size = sum([len(s) + 1 for s in data])\n",
    "        \n",
    "    while True:\n",
    "        # Use shuffle so the order in each epoch is different\n",
    "        random.shuffle(data)\n",
    "\n",
    "        X, Y = [], []\n",
    "        for d in data:\n",
    "            encodes = [re_vocab[START], re_vocab[START]] +\\\n",
    "                      [to_label(t) for t in d] +\\\n",
    "                      [re_vocab[END]]\n",
    "            for i in range(len(encodes) - 2):\n",
    "                X.append([encodes[i], encodes[i+1]])\n",
    "                Y.append(encodes[i+2])\n",
    "\n",
    "                if len(X) >= batch_size:\n",
    "                    X = to_embedding(X)\n",
    "                    Y = np.array(Y)\n",
    "                    \n",
    "                    if one_hot:\n",
    "                        Y = to_categorical(Y, num_classes=len(re_vocab))\n",
    "                        \n",
    "                    yield X, Y\n",
    "                    X, Y = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.504701Z",
     "start_time": "2020-03-22T20:39:56.393718Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_FNN(model, max_len=sent_len-1, seed=None):\n",
    "    \"\"\"\n",
    "    For a given FNN model, generate text. If seed is not provided,\n",
    "    use START as initial seed.\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        max_len: int, maximum length of the setence\n",
    "        seed: str, the seed word used to generate the text\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hints:\n",
    "    1. It's a trigram model, what your intial seed look like?\n",
    "    2. The prediction of each state should return a list of probability, use the \n",
    "       `sample_with_weight` function to help you sample the next word.\n",
    "    3. When the word END is sampled, you need to stop the setence. Also use the max_len\n",
    "       to force ending the setence to avoid the program running forever.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = START\n",
    "\n",
    "    result.append(seed)\n",
    "    X = to_embedding([[re_vocab[START], to_label(seed)]])\n",
    "\n",
    "    word = None\n",
    "    tmp = seed\n",
    "    while word != to_label(END) and len(result) <= max_len:\n",
    "        prob = model.predict(X).flatten()\n",
    "        word = sample_with_weight(prob)\n",
    "\n",
    "        result.append(vocab[word])\n",
    "        X = to_embedding([[to_label(tmp), word]])\n",
    "\n",
    "        tmp = word\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.697802Z",
     "start_time": "2020-03-22T20:39:56.510836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'china academic stephens heat korean visiting aust critics taken cfa flights canadian shortfall push fail'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we train the model, let first check if the text generation function\n",
    "# works as expected. Don't worry if the sentence doesn't make any sense.\n",
    "# We haven't trained the model yet!\n",
    "generate_text_FNN(FNN_model, seed=\"china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.801374Z",
     "start_time": "2020-03-22T20:39:56.700970Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity_FNN(model, X, y):\n",
    "    \"\"\"\n",
    "    For a given FNN model, and test data, calcualte the perplexity.\n",
    "    The definition of perplexity is:\n",
    "    \n",
    "    perplexity = exp(- \\sum_i log(P_i) / N)\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        X: np.array(n_sample, n_gram, embedding_dim)\n",
    "        y: np.array(n_sample), int label of the next word    \n",
    "    \"\"\"\n",
    "    perplexity = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hits:\n",
    "        1. First make the prod prediction using the model\n",
    "        2. The probability at the position of y is what you look for\n",
    "    \n",
    "    When you have too much UNK word, you will find the perplexity to be lower, but it doesn't \n",
    "    really mean your model is better, can you think why?\n",
    "    \"\"\"\n",
    "    prob = model.predict(X)\n",
    "    for i in range(len(y)):\n",
    "        perplexity += np.log(prob[i, y[i]])\n",
    "\n",
    "    perplexity = np.exp(-perplexity / len(y))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.989407Z",
     "start_time": "2020-03-22T20:39:56.804347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083.804440170829"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check the perplexity for the untrained model\n",
    "# Is your value close to the number of vocabulary? \n",
    "# Is this a coincidence?\n",
    "X_dev_FNN, y_dev_FNN = next(gen_sample_FNN(headlines_dev, batch_size=-1, one_hot=False))\n",
    "\n",
    "calculate_perplexity_FNN(FNN_model, X_dev_FNN, y_dev_FNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to: $$perplexity = e^\\frac{-\\sum_i{log{\\frac{1}{P_i}}}}{N}$$ and $$P_i = \\frac{1}{vocab_size}$$.\n",
    "We can have $$perplexity = e^{N * \\frac{log{vocab_size}}{N}} = vocab_size$$\n",
    "So it is not a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T23:12:21.429617Z",
     "start_time": "2020-03-15T23:12:21.302324Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:57.098613Z",
     "start_time": "2020-03-22T20:39:56.992714Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use the function defined above to report the model performance\n",
    "after each epoch\n",
    "\"\"\"\n",
    "def on_epoch_end_FNN(epoch, logs):\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        print(generate_text_FNN(FNN_model))\n",
    "    print('Current perplexity on dev data: ', \n",
    "          calculate_perplexity_FNN(FNN_model, X_dev_FNN, y_dev_FNN), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:22.859257Z",
     "start_time": "2020-03-22T20:39:57.101814Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-827cac677a0e>:14: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 145 steps\n",
      "Epoch 1/10\n",
      "139/145 [===========================>..] - ETA: 0s - loss: 6.0818 - accuracy: 0.2146 - top_k_categorical_accuracy: 0.3640----- Generating text after Epoch: 0\n",
      "<START> kids hodges off to the players <END>\n",
      "<START> new boost in sex <END>\n",
      "<START> bike returns may need hits with insurance <END>\n",
      "Current perplexity on dev data:  199.72002610017705 \n",
      "\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 6.0627 - accuracy: 0.2152 - top_k_categorical_accuracy: 0.3647\n",
      "Epoch 2/10\n",
      "143/145 [============================>.] - ETA: 0s - loss: 5.4016 - accuracy: 0.2410 - top_k_categorical_accuracy: 0.3921----- Generating text after Epoch: 1\n",
      "<START> truck sees ocean of failing trial <END>\n",
      "<START> act govt general problem to be as community <END>\n",
      "<START> smuggling falls <END>\n",
      "Current perplexity on dev data:  171.1996643183982 \n",
      "\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 5.3991 - accuracy: 0.2412 - top_k_categorical_accuracy: 0.3923\n",
      "Epoch 3/10\n",
      "142/145 [============================>.] - ETA: 0s - loss: 5.1974 - accuracy: 0.2466 - top_k_categorical_accuracy: 0.4020----- Generating text after Epoch: 2\n",
      "<START> lack and melbourne stands to build farmers broken into profile <END>\n",
      "<START> interview promises as teen in <END>\n",
      "<START> crews travel term good for more on showdown option on <END>\n",
      "Current perplexity on dev data:  149.3285194510116 \n",
      "\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 5.1946 - accuracy: 0.2465 - top_k_categorical_accuracy: 0.4021\n",
      "Epoch 4/10\n",
      "141/145 [============================>.] - ETA: 0s - loss: 5.0346 - accuracy: 0.2507 - top_k_categorical_accuracy: 0.4122----- Generating text after Epoch: 3\n",
      "<START> new keep in <END>\n",
      "<START> 9 in chris speaks sparks say to save budget <END>\n",
      "<START> aussie signs helping festival at harbour honour for new recovery <END>\n",
      "Current perplexity on dev data:  135.21902921448964 \n",
      "\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 5.0301 - accuracy: 0.2507 - top_k_categorical_accuracy: 0.4125\n",
      "Epoch 5/10\n",
      "142/145 [============================>.] - ETA: 0s - loss: 4.8883 - accuracy: 0.2547 - top_k_categorical_accuracy: 0.4223----- Generating text after Epoch: 4\n",
      "<START> putin coach with intelligence up safety <END>\n",
      "<START> it committee <END>\n",
      "<START> clean team coach <END>\n",
      "Current perplexity on dev data:  127.36537265860001 \n",
      "\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 4.8877 - accuracy: 0.2547 - top_k_categorical_accuracy: 0.4220\n",
      "Epoch 6/10\n",
      "140/145 [===========================>..] - ETA: 0s - loss: 4.7604 - accuracy: 0.2590 - top_k_categorical_accuracy: 0.4304----- Generating text after Epoch: 5\n",
      "<START> wa country claim merger flights game <END>\n",
      "<START> costello official jailed over delay <END>\n",
      "<START> rare passenger <END>\n",
      "Current perplexity on dev data:  120.86164980062067 \n",
      "\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 4.7557 - accuracy: 0.2591 - top_k_categorical_accuracy: 0.4307\n",
      "Epoch 7/10\n",
      "141/145 [============================>.] - ETA: 0s - loss: 4.6412 - accuracy: 0.2616 - top_k_categorical_accuracy: 0.4394----- Generating text after Epoch: 6\n",
      "<START> fish wants needed to shut at the fight <END>\n",
      "<START> cats mine says to life the flood <END>\n",
      "<START> inquiry needs for england for accommodation sale <END>\n",
      "Current perplexity on dev data:  120.04015111642293 \n",
      "\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 4.6358 - accuracy: 0.2621 - top_k_categorical_accuracy: 0.4400\n",
      "Epoch 8/10\n",
      "142/145 [============================>.] - ETA: 0s - loss: 4.5256 - accuracy: 0.2661 - top_k_categorical_accuracy: 0.4491----- Generating text after Epoch: 7\n",
      "<START> woman dies after waiting down at brazil <END>\n",
      "<START> police son wait <END>\n",
      "<START> mayors groups to ground fails to continues davis <END>\n",
      "Current perplexity on dev data:  118.70999181040031 \n",
      "\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 4.5238 - accuracy: 0.2661 - top_k_categorical_accuracy: 0.4490\n",
      "Epoch 9/10\n",
      "142/145 [============================>.] - ETA: 0s - loss: 4.4222 - accuracy: 0.2684 - top_k_categorical_accuracy: 0.4586----- Generating text after Epoch: 8\n",
      "<START> shares down <END>\n",
      "<START> hope targeted out of baghdad asian time <END>\n",
      "<START> wine dollar economy <END>\n",
      "Current perplexity on dev data:  124.9580984928923 \n",
      "\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 4.4205 - accuracy: 0.2683 - top_k_categorical_accuracy: 0.4587\n",
      "Epoch 10/10\n",
      "142/145 [============================>.] - ETA: 0s - loss: 4.3251 - accuracy: 0.2737 - top_k_categorical_accuracy: 0.4674----- Generating text after Epoch: 9\n",
      "<START> farewell markets <END>\n",
      "<START> pakistan rally a drug groups for new of the adelaide city facility <END>\n",
      "<START> albany website contract makes system to be one to report <END>\n",
      "Current perplexity on dev data:  122.37260972972547 \n",
      "\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 4.3214 - accuracy: 0.2738 - top_k_categorical_accuracy: 0.4679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6d46301d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "\"\"\"\n",
    "Notice how the metrics / generated text evolve after each epoch\n",
    "\"\"\"\n",
    "FNN_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "batch_size = 512\n",
    "steps_per_epoch = sum([len(s) + 1 for s in headlines_train]) // batch_size + 1\n",
    "FNN_model.fit_generator(gen_sample_FNN(headlines_train, batch_size=batch_size), \n",
    "                        epochs = 10, steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=[LambdaCallback(on_epoch_end=on_epoch_end_FNN)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:24.046904Z",
     "start_time": "2020-03-22T20:41:22.861780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 15, 128)           219648    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 15, 3152)          406608    \n",
      "=================================================================\n",
      "Total params: 626,256\n",
      "Trainable params: 626,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Activation, TimeDistributed\n",
    "\n",
    "# For simplicity, we use the embedding of words to feed the model, therefore\n",
    "# no need to add a Embedding layer in the begining. But for a possibly better performance\n",
    "# you can add a embedding layer, even better if you use the glove embedding matrix as the\n",
    "# initial value for the embedding layer\n",
    "# This is useful also because we have filled the embedding with random values for those missing\n",
    "# vocabularies, allowing the embedding matrix to relax during training will improve the performance \n",
    "# for these words as well. But be prepared that this would slow down the training\n",
    "\n",
    "# Unfortunately Keras does not have an easy way to support dynamic length of input for RNN model.\n",
    "# So we use the sent_len to truncate all the sentences.\n",
    "batch_size = 10\n",
    "RNN_train_model = Sequential()\n",
    "RNN_train_model.add(\n",
    "    LSTM(128, input_shape=(sent_len, glove.shape[1]), return_sequences=True)\n",
    "    )\n",
    "RNN_train_model.add(TimeDistributed(Dense(len(vocab), activation='softmax')))\n",
    "RNN_train_model.summary()\n",
    "# show_keras_model(RNN_train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.068865Z",
     "start_time": "2020-03-22T20:41:24.051334Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (1, 1, 128)               219648    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (1, 1, 3152)              406608    \n",
      "=================================================================\n",
      "Total params: 626,256\n",
      "Trainable params: 626,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Activation, TimeDistributed\n",
    "\n",
    "\"\"\"\n",
    "# It's tricky to explain why we need the RNN_pred_model. \n",
    "# The RNN_train_model.predict requires a fix length of input (sent_len in our case).\n",
    "# This is not convenient for us because we need to generate the next text one by one.\n",
    "# The trick we play here is to create a shadow model having only 1 time step. We will\n",
    "# copy the parameter of the RNN_train_model to this model once it's trained.\n",
    "# Check generate_text_RNN function to understand details, and there is some discussion \n",
    "# here: \"https://github.com/keras-team/keras/issues/8771\"\n",
    "\"\"\"\n",
    "\n",
    "RNN_pred_model = Sequential()\n",
    "RNN_pred_model.add(\n",
    "    LSTM(128, batch_input_shape=(1, 1, glove.shape[1]), return_sequences=True)\n",
    "    )\n",
    "RNN_pred_model.add(TimeDistributed(Dense(len(vocab), activation='softmax')))\n",
    "RNN_pred_model.summary()\n",
    "# show_keras_model(RNN_pred_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.246426Z",
     "start_time": "2020-03-22T20:41:25.074446Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def gen_sample_RNN(data, batch_size=100, one_hot=True):\n",
    "    \"\"\"\n",
    "    The input is the same to the FNN model, but the output training data is different.\n",
    "    \n",
    "    inputs:\n",
    "        data: list of list of string\n",
    "        batch_size: int\n",
    "        one_hot: boolean\n",
    "        \n",
    "    output:\n",
    "        X: np.array(batch_size, sent_len, embedding_dim)\n",
    "        Y: np.array(batch_size, sent_len, ) or np.array(batch_size, sent_len, vocab_size)\n",
    "    \"\"\"\n",
    "    if batch_size == -1:\n",
    "        batch_size = len(data)\n",
    "        \n",
    "    while True:\n",
    "        # Shuffle the data so data order is different for different epochs\n",
    "        random.shuffle(data)\n",
    "\n",
    "        X, Y = [], []\n",
    "        for s in data:\n",
    "            X.append([to_label(START)] + [to_label(t) for t in s])\n",
    "            Y.append([to_label(t) for t in s] + [to_label(END)])\n",
    "            \n",
    "            if len(X) >= batch_size:   \n",
    "                X = pad_sequences(sequences=X, maxlen=sent_len, padding='post', value=to_label(END))\n",
    "                Y = pad_sequences(sequences=Y, maxlen=sent_len, padding='post', value=to_label(END))\n",
    "          \n",
    "                if one_hot: Y = to_categorical(Y, num_classes=len(re_vocab))\n",
    "                \n",
    "                yield to_embedding(X), Y\n",
    "                \n",
    "                X, Y = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.397070Z",
     "start_time": "2020-03-22T20:41:25.257325Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_RNN(model, max_len=sent_len-1, seed=None):\n",
    "    \"\"\"\n",
    "    Use the RNN_pred_model to generate text. Notice how we use the stateful model to generate\n",
    "    the next word one by one. Make sure you fully understand each line of this code. \n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = START\n",
    "        result = []\n",
    "    else:\n",
    "        result = [seed]\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        X = to_embedding([[to_label(seed)]])\n",
    "        idx = sample_with_weight(model.predict(X)[0][0])\n",
    "        \n",
    "        if vocab[idx] == END: break\n",
    "            \n",
    "        seed = vocab[idx]\n",
    "        result.append(seed)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.670440Z",
     "start_time": "2020-03-22T20:41:25.400064Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'china norman yet afghan authority indies easter mans thai fired reacts bravery strikes tasmanian copper'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_RNN(RNN_pred_model, seed=\"china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T00:48:16.334866Z",
     "start_time": "2020-03-22T00:48:16.220610Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.835637Z",
     "start_time": "2020-03-22T20:41:25.676315Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity_RNN(model, X, y):\n",
    "    \"\"\"\n",
    "    For a given FNN model, and test data, calcualte the perplexity.\n",
    "    The definition of perplexity is:\n",
    "    \n",
    "    perplexity = exp(- \\sum_i log(P_i) / N)\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        X: np.array(n_sample, sent_len, embedding_dim)\n",
    "        y: np.array(n_sample, sent_len), int labels\n",
    "    \"\"\"\n",
    "    perplexity = 0\n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hits:\n",
    "        1. First make the prod prediction using the RNN_train_model\n",
    "        2. The probability at the position of y is what you look for\n",
    "        3. All sentences have fixed length, meaning a sentence can have multiple padding END at the end\n",
    "           of a sentence. Consider stop counting the perplexity once you hit the first END, otherwise\n",
    "           your perplexity will seem too good.\n",
    "    \n",
    "    When you have too much UNK word, you will find the perplexity to be lower, but it doesn't \n",
    "    really mean your model is better, can you think why?\n",
    "    \"\"\"\n",
    "    N = 0\n",
    "    prob = model.predict(X)\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            tmp = prob[i, j, :]\n",
    "            k = y[i, j]\n",
    "            if k != to_label(END):\n",
    "                perplexity += np.log(prob[i, j, k])\n",
    "                N += 1\n",
    "\n",
    "    perplexity = np.exp(-perplexity / N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:26.330668Z",
     "start_time": "2020-03-22T20:41:25.849151Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154.045225507497"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let check the perplexity for the untrained model\n",
    "# Is your value close to the number of vocabulary? \n",
    "# Is this a coincidence?\n",
    "X_dev_RNN, y_dev_RNN = next(gen_sample_RNN(headlines_dev, batch_size=-1, one_hot=False))\n",
    "\n",
    "calculate_perplexity_RNN(RNN_train_model, X_dev_RNN, y_dev_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Without training, the selection is nearly random and the perplexity is very close to the vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use the function defined above to report the model performance\n",
    "after each epoch\n",
    "\"\"\"\n",
    "def on_epoch_end_RNN(epoch, logs):\n",
    "    RNN_pred_model.set_weights(RNN_train_model.get_weights())\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        print(generate_text_RNN(RNN_pred_model))\n",
    "    print('Current perplexity on dev data: ', \n",
    "          calculate_perplexity_RNN(RNN_train_model, X_dev_RNN, y_dev_RNN), '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notice how the metrics / generated text evolve after each epoch\n",
    "\"\"\"\n",
    "batch_size = 10\n",
    "num_batches = len(headlines_train) // batch_size \n",
    "RNN_train_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "RNN_train_model.fit_generator(gen_sample_RNN(headlines_train, batch_size), num_batches, 3,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end_RNN)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:26.481542Z",
     "start_time": "2020-03-22T20:41:26.334649Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:43:25.495611Z",
     "start_time": "2020-03-22T20:42:34.521146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps\n",
      "Epoch 1/3\n",
      " 989/1000 [============================>.] - ETA: 0s - loss: 2.8687----- Generating text after Epoch: 0\n",
      "forestry knights takeover worry fashion crowds royal independence lifted held corruption machete scotland without\n",
      "can demand 28 monday pilot chemical reserve wife legislation seek near 3 johnson clean\n",
      "interview dog warming response assaulting charged become parliament 50 league vatican woodford driver cautious\n",
      "Current perplexity on dev data:  334.8200293450974 \n",
      "\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.8665\n",
      "Epoch 2/3\n",
      " 989/1000 [============================>.] - ETA: 0s - loss: 2.6345----- Generating text after Epoch: 1\n",
      "us democrats support asio 6 tape criminals divided world ferry in seven poll snow\n",
      "tamworth grant massive grant brisbane flow illawarra professor members tamil firefighters cleared are stage\n",
      "downer wodonga secure offences outrage safety conference global returned of france delays signs neck\n",
      "Current perplexity on dev data:  281.7449917012915 \n",
      "\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.6325\n",
      "Epoch 3/3\n",
      " 993/1000 [============================>.] - ETA: 0s - loss: 2.5284----- Generating text after Epoch: 2\n",
      "budget zealand johnson post games declaration lnp buyers canadian courses sold 70 wont tough\n",
      "experts positions opal middle house homelessness floods next of missing groups whale fishers criticism\n",
      "gippsland hunters uncertainty doubt recruit closed in put code landcare call light cats table\n",
      "Current perplexity on dev data:  227.90135443643936 \n",
      "\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6d458a630>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notice how the metrics / generated text evolve after each epoch\n",
    "\"\"\"\n",
    "batch_size = 10\n",
    "num_batches = len(headlines_train) // batch_size \n",
    "RNN_train_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "RNN_train_model.fit_generator(gen_sample_RNN(headlines_train, batch_size), num_batches, 3,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end_RNN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}