{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 Machine translation with Encoder-Decoder model\n",
    "\n",
    "## Due April 24th, 23:59\n",
    "\n",
    "In this homework, you are first shown an example of encoder-decoder machine translation model for a dummy problem. Make sure you understand how it works. Then you will need to build a similar model for a real machine translation data set. The data set provided in this homework is an italiano-english dataset (perché italiano \n",
    "è mia lingua preferita), but feel free to download your preferred language pari here (http://www.manythings.org/anki/).\n",
    "\n",
    "\n",
    "You are given the following files:\n",
    "- `Machine-Translation.ipynb`: This notebook file\n",
    "- `ita.txt`: Training dataset (see http://www.manythings.org/anki/ to understand the structure)\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "\n",
    "### Deliverables (zip them all)\n",
    "\n",
    "- pdf or html version of your final notebook\n",
    "- Show some translation examples in your notebook\n",
    "- writeup.pdf: Add a short essay discussing the biggest challenges you encounter during this assignment and what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T04:43:53.448693Z",
     "start_time": "2020-04-12T04:43:53.358287Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "\n",
    "from utils.general import show_keras_model\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Translation Problem\n",
    "We are not doing anything real here, rather, we create a dummy problem to demonstrate how easy or hard to use a S2S model for machine translation.\n",
    "\n",
    "The dummy prblem I choose here is to translate datestr like \"Aug-30-1989\" to another format \"1989/08/30\". Sounds easy, isn't it? But think about it, you feel this simple because you have so much prior knowledge. You know the English meaning of \"Aug\", you know the different ways of representing dates, MM-DD-YYYY vs YYYY/MM/DD. But our model starts from absolute ignorance. Imagine you show this problem to a 2-year-old child, how much time does it make for him to figure out the rule? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T04:43:54.185037Z",
     "start_time": "2020-04-12T04:43:54.115850Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "choice = np.random.choice\n",
    "def source_generation(batch=100):\n",
    "    months = choice(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], batch)\n",
    "    days = choice(range(1, 28), batch)\n",
    "    years = choice(range(1990, 2050), batch)\n",
    "    \n",
    "    return [ f\"{m}-{d}-{y}\" for m, d, y in zip(months, days, years)]\n",
    "\n",
    "def translate(src):\n",
    "    if type(src) == str: src = [src]\n",
    "    mmap = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': \"06\", 'Jul': \"07\", \n",
    "            'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "    result = []\n",
    "    for d in src:\n",
    "        m, d, y = d.split('-')\n",
    "        result.append(f\"{y}/{mmap[m]}/{str(d).rjust(2, '0')}\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T04:43:54.411868Z",
     "start_time": "2020-04-12T04:43:54.303091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apr-16-2043', 'Mar-17-2041', 'Feb-11-1997', 'Jan-1-2029', 'Feb-17-2004']\n",
      "['2043/04/16', '2041/03/17', '1997/02/11', '2029/01/01', '2004/02/17']\n"
     ]
    }
   ],
   "source": [
    "# Let's generate some data\n",
    "train_X_raw = source_generation(10000)\n",
    "train_Y_raw = translate(train_X_raw)\n",
    "\n",
    "# Verify the translation\n",
    "print(train_X_raw[:5])\n",
    "print(train_Y_raw[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other dummy tasks\n",
    "\n",
    "You are encouraged to generate your own dummy tasks, for example, what about a simple calculator, can you train your model to understand \"186+95\" equal to \"281\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T04:43:55.295413Z",
     "start_time": "2020-04-12T04:43:55.216976Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_len = 11\n",
    "decoder_input_len = 10\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data transformer\n",
    "\n",
    "As of today, I guess you should be quite familar with what we are doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:09:38.063108Z",
     "start_time": "2020-04-12T05:08:40.300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "char_vocab = list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-/0123456789$^')\n",
    "\n",
    "reverse_vocab = {k:v for v, k in enumerate(char_vocab)}\n",
    "def char_to_num(X_raw, is_encoder=True):\n",
    "    \"\"\"\n",
    "    Translate the raw input to the numerical encoding. We take different treatments for the\n",
    "    encoder inputs and decoder inputs. This is because we need a starter character \"^\" for the \n",
    "    decoder inputs.\n",
    "    \"\"\"\n",
    "    result = [[reverse_vocab[c] for c in sent] for sent in X_raw]\n",
    "    \n",
    "    if(is_encoder):\n",
    "        assert all([len(row) <= encoder_input_len for row in X_raw])\n",
    "        return pad_sequences(sequences=result, maxlen=encoder_input_len, \n",
    "                             padding='post', truncating='post', \n",
    "                             value=reverse_vocab['$'])\n",
    "    else:\n",
    "        assert all([len(row) == decoder_input_len for row in X_raw])\n",
    "        return pad_sequences(sequences=result, maxlen=decoder_input_len+1, \n",
    "                             padding='pre', truncating='post', \n",
    "                             value=reverse_vocab['^'])\n",
    "\n",
    "    return pad_sequences(result)\n",
    "\n",
    "def num_to_char(X):\n",
    "    return [''.join([char_vocab[c] for c in row]) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:33:21.018478Z",
     "start_time": "2020-04-12T05:33:20.295116Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, LSTM, Dense, Bidirectional, Embedding, \n",
    "                          TimeDistributed, Concatenate)\n",
    "\n",
    "\"\"\"\n",
    "Define an input Layer. We use one-hot encoding instead of embedding layer here. Since\n",
    "we are using character based model, embedding may not be necessary, and may not be very \n",
    "helpful neither. Do you know why?\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(encoder_input_len, len(char_vocab)), name=\"Encoder_Input\")\n",
    "# For encoder, we can see the entire sentence at once, so we can use Bidirectional LSTM\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, name=\"Encoder_LSTM\"))\n",
    "# Bidrectional LSTM has 4 states instead of 2, we concatenate them to be comparable\n",
    "# with the decoder LSTM\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_inputs)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(decoder_input_len, len(char_vocab)), name=\"Decoder_Input\")\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "decoder_lstm_outputs = decoder_lstm(decoder_inputs,\n",
    "                                    initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(char_vocab), activation='softmax')\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_lstm_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# show_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:50:32.589121Z",
     "start_time": "2020-04-12T05:33:39.762172Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/75\n",
      "8000/8000 [==============================] - 4s 465us/sample - loss: 4.0495 - accuracy: 0.2400 - val_loss: 3.7099 - val_accuracy: 0.3433\n",
      "Epoch 2/75\n",
      "8000/8000 [==============================] - 0s 15us/sample - loss: 2.9697 - accuracy: 0.3220 - val_loss: 2.3804 - val_accuracy: 0.3457\n",
      "Epoch 3/75\n",
      "8000/8000 [==============================] - 0s 16us/sample - loss: 2.1676 - accuracy: 0.3783 - val_loss: 1.9851 - val_accuracy: 0.3646\n",
      "Epoch 4/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 1.9451 - accuracy: 0.3484 - val_loss: 1.9104 - val_accuracy: 0.3906\n",
      "Epoch 5/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 1.8695 - accuracy: 0.3889 - val_loss: 1.8491 - val_accuracy: 0.3906\n",
      "Epoch 6/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.8126 - accuracy: 0.3948 - val_loss: 1.7936 - val_accuracy: 0.3906\n",
      "Epoch 7/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 1.7640 - accuracy: 0.3944 - val_loss: 1.7436 - val_accuracy: 0.3902\n",
      "Epoch 8/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.7153 - accuracy: 0.3933 - val_loss: 1.6916 - val_accuracy: 0.3886\n",
      "Epoch 9/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 1.6619 - accuracy: 0.4020 - val_loss: 1.6325 - val_accuracy: 0.4182\n",
      "Epoch 10/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 1.5991 - accuracy: 0.4391 - val_loss: 1.5626 - val_accuracy: 0.4666\n",
      "Epoch 11/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.5291 - accuracy: 0.4775 - val_loss: 1.4896 - val_accuracy: 0.4934\n",
      "Epoch 12/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.4549 - accuracy: 0.5209 - val_loss: 1.4088 - val_accuracy: 0.5595\n",
      "Epoch 13/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.3638 - accuracy: 0.5800 - val_loss: 1.3035 - val_accuracy: 0.5964\n",
      "Epoch 14/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.2463 - accuracy: 0.5937 - val_loss: 1.1746 - val_accuracy: 0.6005\n",
      "Epoch 15/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.1252 - accuracy: 0.5990 - val_loss: 1.0682 - val_accuracy: 0.6086\n",
      "Epoch 16/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 1.0390 - accuracy: 0.6109 - val_loss: 1.0027 - val_accuracy: 0.6198\n",
      "Epoch 17/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.9866 - accuracy: 0.6272 - val_loss: 0.9639 - val_accuracy: 0.6367\n",
      "Epoch 18/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.9538 - accuracy: 0.6440 - val_loss: 0.9382 - val_accuracy: 0.6514\n",
      "Epoch 19/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.9291 - accuracy: 0.6540 - val_loss: 0.9154 - val_accuracy: 0.6592\n",
      "Epoch 20/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.9074 - accuracy: 0.6587 - val_loss: 0.8962 - val_accuracy: 0.6613\n",
      "Epoch 21/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.8895 - accuracy: 0.6649 - val_loss: 0.8816 - val_accuracy: 0.6709\n",
      "Epoch 22/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.8725 - accuracy: 0.6729 - val_loss: 0.8653 - val_accuracy: 0.6754\n",
      "Epoch 23/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.8560 - accuracy: 0.6798 - val_loss: 0.8482 - val_accuracy: 0.6805\n",
      "Epoch 24/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.8396 - accuracy: 0.6862 - val_loss: 0.8301 - val_accuracy: 0.6900\n",
      "Epoch 25/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.8213 - accuracy: 0.6947 - val_loss: 0.8131 - val_accuracy: 0.6995\n",
      "Epoch 26/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.8062 - accuracy: 0.7026 - val_loss: 0.7955 - val_accuracy: 0.7082\n",
      "Epoch 27/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.7860 - accuracy: 0.7147 - val_loss: 0.7778 - val_accuracy: 0.7185\n",
      "Epoch 28/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.7656 - accuracy: 0.7250 - val_loss: 0.7565 - val_accuracy: 0.7292\n",
      "Epoch 29/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.7451 - accuracy: 0.7351 - val_loss: 0.7354 - val_accuracy: 0.7398\n",
      "Epoch 30/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.7245 - accuracy: 0.7447 - val_loss: 0.7171 - val_accuracy: 0.7462\n",
      "Epoch 31/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.7043 - accuracy: 0.7523 - val_loss: 0.6931 - val_accuracy: 0.7593\n",
      "Epoch 32/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.6875 - accuracy: 0.7565 - val_loss: 0.6732 - val_accuracy: 0.7651\n",
      "Epoch 33/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.6654 - accuracy: 0.7638 - val_loss: 0.6522 - val_accuracy: 0.7723\n",
      "Epoch 34/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.6428 - accuracy: 0.7727 - val_loss: 0.6319 - val_accuracy: 0.7791\n",
      "Epoch 35/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.6338 - accuracy: 0.7722 - val_loss: 0.6149 - val_accuracy: 0.7839\n",
      "Epoch 36/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.6155 - accuracy: 0.7788 - val_loss: 0.5999 - val_accuracy: 0.7891\n",
      "Epoch 37/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.5910 - accuracy: 0.7896 - val_loss: 0.5832 - val_accuracy: 0.7939\n",
      "Epoch 38/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.5747 - accuracy: 0.7943 - val_loss: 0.5694 - val_accuracy: 0.7975\n",
      "Epoch 39/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.5610 - accuracy: 0.7990 - val_loss: 0.5511 - val_accuracy: 0.8051\n",
      "Epoch 40/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.5443 - accuracy: 0.8040 - val_loss: 0.5390 - val_accuracy: 0.8065\n",
      "Epoch 41/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.5313 - accuracy: 0.8095 - val_loss: 0.5254 - val_accuracy: 0.8131\n",
      "Epoch 42/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.5190 - accuracy: 0.8132 - val_loss: 0.5137 - val_accuracy: 0.8152\n",
      "Epoch 43/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.5076 - accuracy: 0.8174 - val_loss: 0.5008 - val_accuracy: 0.8224\n",
      "Epoch 44/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4966 - accuracy: 0.8220 - val_loss: 0.4898 - val_accuracy: 0.8255\n",
      "Epoch 45/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4972 - accuracy: 0.8197 - val_loss: 0.4835 - val_accuracy: 0.8241\n",
      "Epoch 46/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4909 - accuracy: 0.8189 - val_loss: 0.4780 - val_accuracy: 0.8278\n",
      "Epoch 47/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4730 - accuracy: 0.8274 - val_loss: 0.4658 - val_accuracy: 0.8316\n",
      "Epoch 48/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4586 - accuracy: 0.8347 - val_loss: 0.4525 - val_accuracy: 0.8414\n",
      "Epoch 49/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4476 - accuracy: 0.8416 - val_loss: 0.4416 - val_accuracy: 0.8404\n",
      "Epoch 50/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4375 - accuracy: 0.8431 - val_loss: 0.4328 - val_accuracy: 0.8448\n",
      "Epoch 51/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.4285 - accuracy: 0.8471 - val_loss: 0.4217 - val_accuracy: 0.8520\n",
      "Epoch 52/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4383 - accuracy: 0.8392 - val_loss: 0.5086 - val_accuracy: 0.8152\n",
      "Epoch 53/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4844 - accuracy: 0.8189 - val_loss: 0.5136 - val_accuracy: 0.8067\n",
      "Epoch 54/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4625 - accuracy: 0.8250 - val_loss: 0.4460 - val_accuracy: 0.8353\n",
      "Epoch 55/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.4294 - accuracy: 0.8403 - val_loss: 0.4180 - val_accuracy: 0.8478\n",
      "Epoch 56/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.4113 - accuracy: 0.8496 - val_loss: 0.4036 - val_accuracy: 0.8532\n",
      "Epoch 57/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3970 - accuracy: 0.8569 - val_loss: 0.3904 - val_accuracy: 0.8600\n",
      "Epoch 58/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3866 - accuracy: 0.8620 - val_loss: 0.3812 - val_accuracy: 0.8653\n",
      "Epoch 59/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.3775 - accuracy: 0.8668 - val_loss: 0.3715 - val_accuracy: 0.8680\n",
      "Epoch 60/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3689 - accuracy: 0.8712 - val_loss: 0.3634 - val_accuracy: 0.8746\n",
      "Epoch 61/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3603 - accuracy: 0.8751 - val_loss: 0.3560 - val_accuracy: 0.8769\n",
      "Epoch 62/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3532 - accuracy: 0.8774 - val_loss: 0.3481 - val_accuracy: 0.8806\n",
      "Epoch 63/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3513 - accuracy: 0.8768 - val_loss: 0.3770 - val_accuracy: 0.8584\n",
      "Epoch 64/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.3561 - accuracy: 0.8721 - val_loss: 0.3488 - val_accuracy: 0.8745\n",
      "Epoch 65/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.3379 - accuracy: 0.8815 - val_loss: 0.3304 - val_accuracy: 0.8868\n",
      "Epoch 66/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.3269 - accuracy: 0.8880 - val_loss: 0.3228 - val_accuracy: 0.8888\n",
      "Epoch 67/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3248 - accuracy: 0.8876 - val_loss: 0.3179 - val_accuracy: 0.8934\n",
      "Epoch 68/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3193 - accuracy: 0.8891 - val_loss: 0.3166 - val_accuracy: 0.8894\n",
      "Epoch 69/75\n",
      "8000/8000 [==============================] - 0s 14us/sample - loss: 0.3078 - accuracy: 0.8949 - val_loss: 0.3029 - val_accuracy: 0.8996\n",
      "Epoch 70/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3005 - accuracy: 0.8982 - val_loss: 0.3015 - val_accuracy: 0.8971\n",
      "Epoch 71/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.3122 - accuracy: 0.8896 - val_loss: 0.3119 - val_accuracy: 0.8843\n",
      "Epoch 72/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.2975 - accuracy: 0.8949 - val_loss: 0.2841 - val_accuracy: 0.9061\n",
      "Epoch 73/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.2824 - accuracy: 0.9059 - val_loss: 0.2772 - val_accuracy: 0.9087\n",
      "Epoch 74/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.2757 - accuracy: 0.9091 - val_loss: 0.2732 - val_accuracy: 0.9083\n",
      "Epoch 75/75\n",
      "8000/8000 [==============================] - 0s 13us/sample - loss: 0.2664 - accuracy: 0.9140 - val_loss: 0.2628 - val_accuracy: 0.9158\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\"\"\"\n",
    "Don't be suprized that this model actually needs quite quite a lot of epochs to train, so please be patient.\n",
    "After the model is trained, you can use the history.history object to plot the metrics improvment process.\n",
    "\n",
    "While you are waiting for the model to train, feel free to read the next cell.\n",
    "\"\"\"\n",
    "batch_size = 1000\n",
    "epochs = 75\n",
    "\n",
    "# Here it's just some data transformation to translate the raw data to matrix inputs\n",
    "encoder_input_data = to_categorical(char_to_num(train_X_raw, True), num_classes=len(char_vocab))\n",
    "train_Y = to_categorical(char_to_num(train_Y_raw, False), num_classes=len(char_vocab))\n",
    "# for decoder, the target lags input by 1 time step\n",
    "decoder_input_data = train_Y[:, :-1, :]\n",
    "decoder_target_data = train_Y[:, 1:, :]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference model\n",
    "\n",
    "Similar to HW04, we need a different model structure for the inference model. The inference model should copy exactly the same weights from the training model, but it predicts only 1 time step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:50:33.010402Z",
     "start_time": "2020-04-12T05:50:32.598875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trucate the encoder part of the training model as encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# show_keras_model(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:50:34.888151Z",
     "start_time": "2020-04-12T05:50:33.988906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the inference model\n",
    "inference_inputs = Input(batch_shape=(1,1, len(char_vocab)), name=\"Inference_Input\")\n",
    "inference_lstm = LSTM(latent_dim*2, stateful=True,\n",
    "                      name=\"Inference_LSTM\")\n",
    "inference_lstm_outputs = inference_lstm(inference_inputs)\n",
    "\n",
    "inference_dense = Dense(len(char_vocab), activation='softmax')\n",
    "inference_outputs = inference_dense(inference_lstm_outputs)\n",
    "\n",
    "# Assign the weights of decoder to inference model\n",
    "inference_lstm.set_weights(decoder_lstm.get_weights())\n",
    "inference_dense.set_weights(decoder_dense.get_weights())\n",
    "\n",
    "inference_model = Model(inference_inputs, inference_outputs)\n",
    "# show_keras_model(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:50:36.046828Z",
     "start_time": "2020-04-12T05:50:35.913781Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(encoder_input_data):\n",
    "    \"\"\"\n",
    "    A utility function to generate the model prediction\n",
    "    \"\"\"\n",
    "    states_h, states_c = encoder_model.predict(encoder_input_data)\n",
    "    results = []\n",
    "    \n",
    "    for h, c in zip(states_h, states_c):\n",
    "        sent, seed = [], reverse_vocab['^']\n",
    "        inference_lstm.states[0].assign(h[None, :])\n",
    "        inference_lstm.states[1].assign(c[None, :])\n",
    "        for i in range(decoder_input_len):\n",
    "            seed = to_categorical(np.array([seed]), num_classes=len(char_vocab))[None, :, :]\n",
    "            seed = inference_model.predict(seed)[0].argmax()\n",
    "            sent.append(seed)\n",
    "            \n",
    "        results.append(sent)\n",
    "        \n",
    "    return num_to_char(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T05:50:36.412556Z",
     "start_time": "2020-04-12T05:50:36.050976Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apr-16-2043', 'Mar-17-2041', 'Feb-11-1997', 'Jan-1-2029$', 'Feb-17-2004', 'Oct-27-2013', 'Aug-1-2021$', 'Jul-13-1995', 'May-21-2017', 'Feb-20-2009']\n",
      "['2043/04/16', '2041/04/15', '1997/02/11', '2029/01/09', '2004/02/16', '2013/10/27', '2012/08/01', '1995/07/10', '2017/06/12', '2009/02/20']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at some output\n",
    "print(num_to_char(encoder_input_data[:10].argmax(axis=2)))\n",
    "print(inference(encoder_input_data[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Machine translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T02:12:12.716315Z",
     "start_time": "2020-04-17T02:12:12.324229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now are you ready for the real challenge? You can use the ita.txt file as training data. \n",
    "But feel free to download different language from http://www.manythings.org/anki/. If you\n",
    "happen to speak French or Japanese, it's time to show off!\n",
    "\n",
    "1. Implement a Bidrectional LSTM Encoder-Decoder model, or other viable models to translate \n",
    "   the language dataset you choose.\n",
    "\n",
    "2. Write the function to calculate the BLEU score of your model\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "lines= pd.read_csv('spa.txt', names=['eng', 'spa', 'source'], sep='\\t')\n",
    "\n",
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.spa=lines.spa.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.spa=lines.spa.apply(lambda x: re.sub(\"'\", '', x))\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.spa=lines.spa.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.spa = lines.spa.apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.spa=lines.spa.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.spa=lines.spa.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "start_token = '<START> '\n",
    "end_token = '<END>'\n",
    "lines.spa = lines.spa.apply(lambda x : ''.join([start_token, x, end_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>spa</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52543</th>\n",
       "      <td>this word comes from latin</td>\n",
       "      <td>&lt;START&gt; esta palabra viene del latín&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69668</th>\n",
       "      <td>tom doesnt remember very much</td>\n",
       "      <td>&lt;START&gt; tom no recuerda tanto&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119350</th>\n",
       "      <td>sometimes we do what we have to do not what we...</td>\n",
       "      <td>&lt;START&gt; a veces hacemos lo que debemos hacer n...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94371</th>\n",
       "      <td>i fell in love with her at first sight</td>\n",
       "      <td>&lt;START&gt; me enamoré de ella a la primera vista&lt;...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18884</th>\n",
       "      <td>tom got mary drunk</td>\n",
       "      <td>&lt;START&gt; tom emborrachó a mary&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71848</th>\n",
       "      <td>i have never been to the states</td>\n",
       "      <td>&lt;START&gt; no he estado nunca en los estados unid...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>try to be brave tom</td>\n",
       "      <td>&lt;START&gt; trata de ser valiente tom&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85928</th>\n",
       "      <td>i can assure you that you are wrong</td>\n",
       "      <td>&lt;START&gt; te puedo asegurar que estás equivocada...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35495</th>\n",
       "      <td>tom must be very tired</td>\n",
       "      <td>&lt;START&gt; tom debe estar muy cansado&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110788</th>\n",
       "      <td>her old bike squeaked as she rode down the hill</td>\n",
       "      <td>&lt;START&gt; su vieja bicicleta chirrió mientras ba...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "52543                          this word comes from latin   \n",
       "69668                       tom doesnt remember very much   \n",
       "119350  sometimes we do what we have to do not what we...   \n",
       "94371              i fell in love with her at first sight   \n",
       "18884                                  tom got mary drunk   \n",
       "71848                     i have never been to the states   \n",
       "27095                                 try to be brave tom   \n",
       "85928                 i can assure you that you are wrong   \n",
       "35495                              tom must be very tired   \n",
       "110788    her old bike squeaked as she rode down the hill   \n",
       "\n",
       "                                                      spa  \\\n",
       "52543           <START> esta palabra viene del latín<END>   \n",
       "69668                  <START> tom no recuerda tanto<END>   \n",
       "119350  <START> a veces hacemos lo que debemos hacer n...   \n",
       "94371   <START> me enamoré de ella a la primera vista<...   \n",
       "18884                  <START> tom emborrachó a mary<END>   \n",
       "71848   <START> no he estado nunca en los estados unid...   \n",
       "27095              <START> trata de ser valiente tom<END>   \n",
       "85928   <START> te puedo asegurar que estás equivocada...   \n",
       "35495             <START> tom debe estar muy cansado<END>   \n",
       "110788  <START> su vieja bicicleta chirrió mientras ba...   \n",
       "\n",
       "                                                   source  \n",
       "52543   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "69668   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "119350  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "94371   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "18884   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "71848   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "27095   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "85928   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "35495   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "110788  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of French \n",
    "all_spa_words=set()\n",
    "for spa in lines.spa:\n",
    "    for word in spa.split():\n",
    "        if word not in all_spa_words:\n",
    "            all_spa_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.spa:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13475, 36608)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_spa_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_spa_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For zero padding\n",
    "num_decoder_tokens += 1\n",
    "num_encoder_tokens += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>spa</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93840</th>\n",
       "      <td>do you see that house thats my house</td>\n",
       "      <td>&lt;START&gt; ¿ves aquella casa esa es mi casa&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95883</th>\n",
       "      <td>unfortunately there was no one around</td>\n",
       "      <td>&lt;START&gt; desafortunadamente no había ninguna pe...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68739</th>\n",
       "      <td>please have someone else do it</td>\n",
       "      <td>&lt;START&gt; por favor que alguien más lo haga&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18342</th>\n",
       "      <td>she was in a hurry</td>\n",
       "      <td>&lt;START&gt; ella estaba apresurada&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>dogs are smart</td>\n",
       "      <td>&lt;START&gt; los perros son inteligentes&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68818</th>\n",
       "      <td>she carried a baby on her back</td>\n",
       "      <td>&lt;START&gt; ella cargó un bebé en su espalda&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6478</th>\n",
       "      <td>i was an idiot</td>\n",
       "      <td>&lt;START&gt; era un idiota&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80640</th>\n",
       "      <td>steam is coming out of the engine</td>\n",
       "      <td>&lt;START&gt; está saliendo vapor del motor&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44026</th>\n",
       "      <td>tom cant find his shoes</td>\n",
       "      <td>&lt;START&gt; tom no encuentra sus zapatos&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47697</th>\n",
       "      <td>she is at home in english</td>\n",
       "      <td>&lt;START&gt; ella es hábil en el inglés&lt;END&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         eng  \\\n",
       "93840   do you see that house thats my house   \n",
       "95883  unfortunately there was no one around   \n",
       "68739         please have someone else do it   \n",
       "18342                     she was in a hurry   \n",
       "5969                          dogs are smart   \n",
       "68818         she carried a baby on her back   \n",
       "6478                          i was an idiot   \n",
       "80640      steam is coming out of the engine   \n",
       "44026                tom cant find his shoes   \n",
       "47697              she is at home in english   \n",
       "\n",
       "                                                     spa  \\\n",
       "93840      <START> ¿ves aquella casa esa es mi casa<END>   \n",
       "95883  <START> desafortunadamente no había ninguna pe...   \n",
       "68739     <START> por favor que alguien más lo haga<END>   \n",
       "18342                <START> ella estaba apresurada<END>   \n",
       "5969            <START> los perros son inteligentes<END>   \n",
       "68818      <START> ella cargó un bebé en su espalda<END>   \n",
       "6478                          <START> era un idiota<END>   \n",
       "80640         <START> está saliendo vapor del motor<END>   \n",
       "44026          <START> tom no encuentra sus zapatos<END>   \n",
       "47697            <START> ella es hábil en el inglés<END>   \n",
       "\n",
       "                                                  source  \n",
       "93840  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "95883  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "68739  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "18342  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "5969   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "68818  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "6478   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "80640  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "44026  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "47697  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111393,), (12377,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.spa\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j: j + batch_size], y[j: j + batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t < len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t > 0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "                        \n",
    "            yield [encoder_input_data, decoder_input_data], decoder_target_data, [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove = pd.read_csv(\"glove_6B_100d_top100k.csv\")\n",
    "\n",
    "embedding_matrix = np.zeros((num_encoder_tokens, 100))\n",
    "for word, i in input_token_index.items():\n",
    "    if word in glove.columns:\n",
    "        embedding_matrix[i] = glove.loc[:, word].to_numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name=\"Encoder_Input\")\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "\n",
    "# For encoder, we can see the entire sentence at once, so we can use Bidirectional LSTM\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, name=\"Encoder_LSTM\"))\n",
    "# Bidrectional LSTM has 4 states instead of 2, we concatenate them to be comparable\n",
    "# with the decoder LSTM\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_emb)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,), name=\"Decoder_Input\")\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-164175582aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     validation_steps = val_samples // batch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The kernel keeps crashing. So I continue my work on .py file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= Embedding(num_decoder_tokens, latent_dim)(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index[start_token]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == end_token or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "import nltk\n",
    "\n",
    "hypothesis = decoded_sentence\n",
    "reference = actual_output\n",
    "#there may be several references\n",
    "BLEU_score = nltk.translate.bleu_score.sentence_bleu(reference, hypothesis)\n",
    "\n",
    "print(BLEU_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
